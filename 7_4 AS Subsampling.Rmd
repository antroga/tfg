---
title: "Aprendizaje supervisado"
output: html_notebook
---

En las experimentaciones anteriores se han utilizado tres algoritmos distintos para aprendizaje supervisado (árbol de decisión, Xgboost y SVM). 
El hecho de que las clases de clasificación estén muy desbalanceadas hace que los modelos generados no clasifiquen como se espera.


Así, en este notebook se trabaja con los predictores y la clases Y1(ser nodo aislado) e Y2 (estar en NCG), intentando solventar el problema del desbalance.

En primer lugar se cargan las librerías necesarias.
```{r}
library(dplyr)
library(igraph)
library(readxl)
library(rpart)
library(rpart.plot)
library(e1071)
library(xgboost)
library(caret)
```

Cargo el dataframe
```{r}
grafodf <- read.table("./grafodf.txt", header = TRUE, sep = ",")
```
Cambio los nombres de las columnas para facilitar la aplicación de los algoritmos.
```{r}
colnames(grafodf) <- c("ID","NODOS", "X1","X2", "X3", "X4", "Y1", "Y2" )
```
Como puede verse en el histograma, las clases de clasificación están muy desbalanceadas.
```{r}
hist(grafodf$Y1)
hist(grafodf$Y2)
```
Para aplicar el algoritmo, es necesario que las variables de respuesta sean factores.
```{r}
grafodf$Y1 <- as.factor(grafodf$Y1)
grafodf$Y2 <- as.factor(grafodf$Y2)
```

Me quedo con los predictores y la etiqueta Y2.
```{r}
df1 <- grafodf[, 3:7]
df2 <- select(grafodf,3,4,5,6, 8) 
```
Divido en datos de entrenamiento y prueba.
```{r}
set.seed(1)
trainIndex1 <- createDataPartition(df1$Y1, p = .75, 
                                  list = FALSE)
trainY1 <- df1[ trainIndex1,]
testY1  <- df1[-trainIndex1,]

trainIndex2 <- createDataPartition(df2$Y2, p = .75, 
                                  list = FALSE)
trainY2 <- df2[ trainIndex2,]
testY2  <- df2[-trainIndex2,]
```

Para evitar el desbalance, hay dos opciones: "quitar" ejemplos de la clase 0 (no estar en NCG) para que haya una cantidad similar a la clase 1, o estimar nuevos ejemplos que se cataloguen como 1 para compensar.
Esto último es lo que haremos, ya que de lo contrario estaríamos perdiendo mucha información.
```{r}
set.seed(9560)
up_train1 <- upSample(x = trainY1[, -ncol(trainY1)],
                     y = trainY1$Y1)                         
table(up_train1$Class) 
head(up_train1)

up_train2 <- upSample(x = trainY2[, -ncol(trainY2)],
                     y = trainY2$Y2)                         
table(up_train2$Class) 
head(up_train2)
```
Representamos gráficamente los histogramas para ver la distribución de las clases.
```{r}
hist(as.numeric(up_train1$Class))
hist(as.numeric(up_train2$Class))
```

Con esto tenemos un conjunto de entrenamiento balanceado totalmente.
Entrenamos los modelos y evaluamos.

- Árbol de decisión
```{r}
arbol1 <- rpart(Class~., data = up_train1, method = 'class')
rpart.plot(arbol1)

arbol2 <- rpart(Class~., data = up_train2, method = 'class')
rpart.plot(arbol2)
```
Evaluamos los modelos:
- Etiqueta Y1.
```{r}
pred_arbol <- predict(arbol1, testY1, type = 'class')
confusionMatrix(table(pred_arbol, testY1$Y1))
```
- Etiqueta Y2.
```{r}
pred_arbol <- predict(arbol2, testY2, type = 'class')
confusionMatrix(table(pred_arbol, testY2$Y2))
```
Esto con los ajustes por defecto, ahora optimizamos el parámetro CP y entrenamos con k-CV, k =10.
```{r}
control_class <- trainControl(
  method = "cv",
  number = 10
)

cps = seq(0 , 1, by=0.05)
arbol11 <- train (Class ~ ., up_train1,
                 tuneGrid = expand.grid(cp = cps),
                 method="rpart", 
                 trControl=control_class)
print(arbol11)

arbol22 <- train (Class ~ ., up_train2,
                 tuneGrid = expand.grid(cp = cps),
                 method="rpart", 
                 trControl=control_class)
print(arbol22)
```

Predecimos y evaluamos.
- Etiqueta Y1.
```{r}
pred_arbol1 <- predict(arbol11, testY1)
confusionMatrix(table(pred_arbol1, testY1$Y1))
```
- Etiqueta Y2.
```{r}
pred_arbol2 <- predict(arbol22, testY2)
confusionMatrix(table(pred_arbol2, testY2$Y2))
```

- SVM.
Entreno los modelos. 
```{r}
modelo_svm1 <- svm(formula = Class ~ ., data = up_train1, kernel = "linear",
                   scale = FALSE)
modelo_svm2 <- svm(formula = Class ~ ., data = up_train2, kernel = "linear",
                   scale = FALSE)
```
Predecimos y evaluamos.
- Etiqueta Y1.
```{r}
pred <- predict(modelo_svm1, testY1, type = 'class')
confusionMatrix(table(pred, testY1$Y1))
```
- Etiqueta Y2.
```{r}
pred <- predict(modelo_svm2, testY2, type = 'class')
confusionMatrix(table(pred, testY2$Y2))
```
Optimizamos los parámetros.
```{r}
costs = seq(1 , 100, by=10)
svm1 <- train (Class ~ ., up_train1,
                 tuneGrid = expand.grid(cost = costs),
                 method="svmLinear2", 
                 trControl=control_class)
print(svm1)

svm2 <- train (Class ~ ., up_train2,
                 tuneGrid = expand.grid(cost = costs),
                 method="svmLinear2", 
                 trControl=control_class)
print(svm2)
```
Evalúo con el modelo óptimo.
- Etiqueta Y1.
```{r}
pred1 <- predict(svm1, testY1)
confusionMatrix(table(pred1, testY1$Y1))
```
- Etiqueta Y2.
```{r}
pred2 <- predict(svm2, testY2)
confusionMatrix(table(pred2, testY2$Y2))
```
 Se mejoran las predicciones con la optimización.
 

En general, el desempeño de los modelos ha mejorado al estar la clase de entrenamiento balanceada. 

