---
title: "Aprendizaje supervisado: Árbol de decisión"
output: html_notebook
---

En este apartado se lleva a cabo un estudio de tipo supervisado con el algoritmo de árbol de decisión XXX

En primer lugar se instalan los paquetes y se cargan las librerías necesarias.
```{r}
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("gplots")
#install.packages("caret")
library(rpart)
library(rpart.plot)
library(dplyr)
library(caret)
```

Cargo el dataframe
```{r}
grafodf <- read.table("./grafodf.txt", header = TRUE, sep = ",")
```
Cambio los nombres de las columnas para facilitar la aplicación de los algoritmos.
```{r}
colnames(grafodf) <- c( "ID", "NODOS", "X1","X2", "X3", "X4", "Y1", "Y2" )
grafodf
```
Convierto las variables de respuesta en factores.
```{r}
grafodf$Y1 <- as.factor(grafodf$Y1)
grafodf$Y2 <- as.factor(grafodf$Y2)
```

Creo dos conjuntos, uno para cada etiqueta.
```{r}
df1 <- grafodf[, 3:7]
df2 <- select(grafodf,3,4,5,6, 8) 
```

Divido el conjunto de datos en test (75%) y train (25%), para ambas etiquetas.
- Y1.
```{r}
set.seed(3456)

trainIndex1 <- createDataPartition(df1$Y1, p = .75, 
                                  list = FALSE)
trainY1 <- df1[ trainIndex1,]
testY1  <- df1[-trainIndex1,]
```
Visualizamos los conjuntos.
El conjunto de train está formado por 133 ejemplos y el de test, por 44.
```{r}
summary(trainY1)
dim(trainY1)
summary(testY1)
dim(testY1)
```
Comprobamos que las etiquetas siguen una distribución similar al conjunto completo.
```{r}
hist(as.numeric(trainY1$Y1))
hist(as.numeric(testY1$Y1))
```
- Y2.
```{r}
set.seed(1)
trainIndex2 <- createDataPartition(df2$Y2, p = .75, 
                                  list = FALSE)
trainY2 <- df2[ trainIndex2,]
testY2  <- df2[-trainIndex2,]
```
Visualizamos los conjuntos.
El conjunto de train está formado por 133 ejemplos y el de test, por 44.
```{r}
summary(trainY2)
dim(trainY2)
summary(testY2)
dim(testY2)
```
Comprobamos que las etiquetas siguen una distribución similar al conjunto completo.
```{r}
hist(as.numeric(trainY2$Y2))
hist(as.numeric(testY2$Y2))
```

Se construyen los modelos básicos, con los ajustes por defecto.
El árbol para Y1 es muy sencillo, con un solo nodo.
El árbol para Y2 es un poco más complejo.
```{r}
fit1 <- rpart(Y1~., data = trainY1, method = 'class')
rpart.plot(fit1)

fit2 <- rpart(Y2~., data = trainY2, method = 'class')
rpart.plot(fit2)
```
Observamos la construcción de los árboles.
El primero:
```{r}
summary(fit1)
```

El segundo:
```{r}
summary(fit2)
```
Mostramos estadísticas del proceso de construcción de cada árbol.
```{r}
printcp(fit1) 
printcp(fit2)
```


Predecimos y evaluamos el desempeño de los modelos.

- Etiqueta Y1 (ser nodo aislado):
La precisión es 1. Todos los ejemplos se clasifican correctamente, no hay error.
```{r}
pred1 <- predict(fit1, testY1, type = 'class')
confusionMatrix(table(pred1, testY1$Y1))
```

- Etiqueta Y2 (estar en NGC).
La precisión es de 0.88. 
Los ejemplos de la clase 1 se clasifican correctamente, pero algunos de la clase 0 no.
```{r}
pred2 <- predict(fit2, testY2, type = 'class')
confusionMatrix(table(pred2, testY2$Y2))
```

Estos datos, sin embargo, proceden de un único proceso de entrenamiento y validación de los modelos. 
Ahora se hace uso de validación cruzada, con la que se entrena y prueba varias veces el modelo, obteniéndose múltiples estimaciones de error. Si las estimaciones son similares, el modelo será bueno.
Se utiliza K-CV, con k =10.
```{r}
control_class <- trainControl(
  method = "cv",
  number = 10
)
```
También optimizamos los hiperparámetros del Árbol de decisión, probando distintos valores y evaluándolos con validación cruzada. 


El modelo que predice sobre la clase Y1 no puede mejorar puesto que la precisión es 1.
Estos datos, además de la estructura del árbol construido, nos llevan a pensar que la clase en la que se basa la clasificación no es buena. Por ello, no seguiremos haciendo mejoras para los datos con Y1.

Para la etiqueta Y2 (estar en NCG):
Se prueban 10 valores distintos del parámetro principal, cp.
```{r}
cps = seq(0 , 1, by=0.05)
arbol2 <- train (Y2 ~ ., trainY2,
                 tuneGrid = expand.grid(cp = cps),
                 method="rpart", 
                 trControl=control_class)
print(arbol2)
```
El mejor modelo de los entrenados es aquel con cp=1, que consigue un accuracy de 0.880677.

Predecimos usando el modelo óptimo:
```{r}
pred2 <- predict(arbol2, testY2)
confusionMatrix(table(pred2, testY2$Y2))
```

El accuracy es el mismo y el árbol ha perdido calidad (clasifica todo como la clase dominante, 0). La métrica Kappa también denota esto, siendo 0. 
En este caso, la optimización no ha mejorado el modelo. 

